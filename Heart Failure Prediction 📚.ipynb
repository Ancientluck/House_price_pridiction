{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Exploring Data ðŸ“š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np  \n",
    "\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt  \n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"dataset/archive/heart_failure_clinical_records_dataset.csv\") \n",
    "data.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of the data\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Information about data:- \") \n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"describing the data:- \") \n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"No. of Null Values:-\" ) \n",
    "data.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis ðŸ“š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seeing the distribution of classes, this will help us to identify which types \n",
    "\n",
    "len_live = len(data[\"DEATH_EVENT\"][data.DEATH_EVENT == 0])\n",
    "len_death = len(data[\"DEATH_EVENT\"][data.DEATH_EVENT == 1])\n",
    "\n",
    "arr = np.array([len_live , len_death]) \n",
    "labels = ['LIVING', 'DIED'] \n",
    "print(\"Total No. Of Living Cases :- \", len_live)\n",
    "print(\"Total No. Of Died Cases :- \", len_death)\n",
    "\n",
    "plt.pie(arr, labels=labels, explode = [0.2,0.0] , shadow=True) \n",
    "plt.show() \n",
    "\n",
    "# inference :- we are actually working on imbalance data  \n",
    "# imbalance:- your data is not equally distributed between classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seeing the distribution of Age  \n",
    "\n",
    "sns.distplot(data[\"age\"]) \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting rows that are above age 50 and seeing died or not \n",
    "age_above_50_not_died = data['DEATH_EVENT'][data.age >= 50][data.DEATH_EVENT == 0]\n",
    "age_above_50_died = data['DEATH_EVENT'][data.age >= 50][data.DEATH_EVENT == 1]  \n",
    "\n",
    "len_died = len(age_above_50_died) \n",
    "len_not_died = len(age_above_50_not_died) \n",
    "\n",
    "arr1 = [len_died, len_not_died] \n",
    "labels = ['DIED', 'NOT DIED'] \n",
    "\n",
    "plt.pie(arr1, labels=labels, explode = [0.2,0.0] , shadow=True) \n",
    "plt.show()  \n",
    "\n",
    "print (\"Total no. of died cases, \", len_died) \n",
    "print(\"Total no. of not died cases, \", len_not_died)\n",
    "# inference in most of the cases people aged above 50 not died but accordinly if you compare with above \n",
    "# plot you will be seeing that died ration is comparitively higher here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "patient_nhave_diabetes_0 = data['DEATH_EVENT'][data.diabetes == 1][data.DEATH_EVENT == 0] \n",
    "patient_have_diabetes_1 = data['DEATH_EVENT'][data.diabetes == 1][data.DEATH_EVENT == 1]\n",
    "\n",
    "len_d_died = len(patient_have_diabetes_1) \n",
    "len_d_alive = len(patient_nhave_diabetes_0) \n",
    "\n",
    "arr2 = [len_d_died,len_d_alive] \n",
    "labels = ['Died with Diabetes', \"Not Died with Diabetes\"] \n",
    "plt.pie(arr2, labels=labels, explode = [0.2,0.0] , shadow=True) \n",
    "plt.show()  \n",
    "\n",
    "# inference:- here you can see the that the most of the person are alive who have diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# checking the Correlation of our variables  \n",
    "\n",
    "corr = data.corr() \n",
    "plt.subplots(figsize=(15,15))\n",
    "sns.heatmap(corr, annot=True) \n",
    "\n",
    "# interpretation of correlation matrix \n",
    "\n",
    "''' \n",
    " - Each square shows the correlation between the variables on each axis. Correlation ranges from -1 to +1. Values closer to zero means there is no linear trend between the two variables. \n",
    " - The close to 1 the correlation is the more positively correlated they are; that is as one increases so does the other and the closer to 1 the stronger this relationship is.  \n",
    " - A correlation closer to -1 is similar, but instead of both increasing one variable will decrease as the other increases.  \n",
    " - The diagonals are all 1/dark green because those squares are correlating each variable to itself (so it's a perfect correlation). For the rest the larger the number and darker the color the higher the correlation between the two variables.   \n",
    " - The plot is also symmetrical about the diagonal since the same two variables are being paired together in those squares.\n",
    "''' \n",
    "\n",
    "# references:- https://en.wikipedia.org/wiki/Pearson_correlation_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can do the same as here \n",
    "data.corr().style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset developmentðŸ“š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "X = data.drop('DEATH_EVENT', axis=1) \n",
    "y = data[\"DEATH_EVENT\"] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=0) \n",
    "print(\"Shape of the X_train\", X_train.shape) \n",
    "print(\"Shape of the y_train\", y_train.shape) \n",
    "print(\"Shape of the X_test\", X_test.shape) \n",
    "print(\"Shape of the y_test\", y_test.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering ðŸ“š\n",
    "\n",
    "Now, we will do feature engineering, we will add interaction terms, interaction terms are the product of two features, so below \n",
    "is the function prepared for interaction terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_interactions(X):\n",
    "    features = X.columns\n",
    "    m = len(features)\n",
    "    X_int = X.copy(deep=True)\n",
    "\n",
    "    for i in range(m):\n",
    "        \n",
    "        feature_i_name = features[i]\n",
    "        \n",
    "        feature_i_data = X[feature_i_name]\n",
    "        \n",
    "        for j in range(i+1, m):\n",
    "            \n",
    "            feature_j_name = features[j]\n",
    "            feature_j_data = X[feature_j_name]\n",
    "            feature_i_j_name = feature_i_name+\"_x_\"+feature_j_name\n",
    "            X_int[feature_i_j_name] =  feature_i_data * feature_j_data\n",
    "        \n",
    "    return X_int \n",
    "\n",
    "\n",
    "x_train_mod = add_interactions(X_train) \n",
    "x_test_mod  = add_interactions(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building ðŸ“š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "    \n",
    "def evaluating_model(y_test, y_pred):  \n",
    "    ''' \n",
    "    Function for evaluating our models.\n",
    "    '''\n",
    "    print(\"Accuracy Score:- \", accuracy_score(y_test, y_pred)) \n",
    "    print(\"Precision Score:- \", precision_score(y_test, y_pred)) \n",
    "    print(\"Recall Score:- \", recall_score(y_test, y_pred)) \n",
    "    print(\"Confusion Matrix:- \\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building logistic regression model as a baseline model \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "lr_clf = LogisticRegression(max_iter=1000) \n",
    "lr_clf.fit(X_train, y_train) \n",
    "\n",
    "lr_clf_pred = lr_clf.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr_clf.predict(X_test)\n",
    "evaluating_model(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building logistic regression with StandardScaler  \n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "lr_clf_pip = make_pipeline(StandardScaler(), LogisticRegression()) \n",
    "lr_clf_pip.fit(X_train, y_train) \n",
    "\n",
    "y_pred1 = lr_clf_pip.predict(X_test)\n",
    "evaluating_model(y_test,y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC \n",
    "from sklearn.model_selection import GridSearchCV \n",
    "\n",
    "# defining parameter range\n",
    "param_grid = {'C': [0.1, 1, 10, 100, 1000], \n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "              'kernel': ['rbf']} \n",
    "grid = GridSearchCV(SVC(), param_grid, refit=True, verbose =3) \n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svc = SVC(C = 10, gamma = 0.0001) \n",
    "svc.fit(X_train, y_train) \n",
    "y_pred2 = svc.predict(X_test) \n",
    "evaluating_model(y_test, y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV \n",
    "\n",
    "def randomized_search(params, runs=20, clf=DecisionTreeClassifier(random_state=2)): \n",
    "    rand_clf = RandomizedSearchCV(clf, params, n_iter=runs, cv=5, n_jobs=-1, random_state=2) \n",
    "    rand_clf.fit(X_train, y_train) \n",
    "    best_model = rand_clf.best_estimator_\n",
    "    best_score = rand_clf.best_score_\n",
    "\n",
    "    print(\"Training score: {:.3f}\".format(best_score))\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print('Test score: {:.3f}'.format(accuracy))\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "randomized_search(params={'criterion':['entropy', 'gini'],\n",
    "                              'splitter':['random', 'best'],\n",
    "                          'min_weight_fraction_leaf':[0.0, 0.0025, 0.005, 0.0075, 0.01],\n",
    "                          'min_samples_split':[2, 3, 4, 5, 6, 8, 10],\n",
    "                          'min_samples_leaf':[1, 0.01, 0.02, 0.03, 0.04],\n",
    "                          'min_impurity_decrease':[0.0, 0.0005, 0.005, 0.05, 0.10, 0.15, 0.2],\n",
    "                          'max_leaf_nodes':[10, 15, 20, 25, 30, 35, 40, 45, 50, None],\n",
    "                          'max_features':['auto', 0.95, 0.90, 0.85, 0.80, 0.75, 0.70],\n",
    "                          'max_depth':[None, 2,4,6,8],\n",
    "                          'min_weight_fraction_leaf':[0.0, 0.0025, 0.005, 0.0075, 0.01, 0.05]\n",
    "                         })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_clf = DecisionTreeClassifier(max_depth=8, max_features=0.9, max_leaf_nodes=30,\n",
    "                       min_impurity_decrease=0.05, min_samples_leaf=0.02,\n",
    "                       min_samples_split=10, min_weight_fraction_leaf=0.005,\n",
    "                       random_state=2, splitter='random') \n",
    "ds_clf.fit(X_train, y_train) \n",
    "pred4 = ds_clf.predict(X_test) \n",
    "evaluating_model(y_test, pred4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "randomized_search(params={\n",
    "                         'min_samples_leaf':[1,2,4,6,8,10,20,30],\n",
    "                          'min_impurity_decrease':[0.0, 0.01, 0.05, 0.10, 0.15, 0.2],\n",
    "                          'max_features':['auto', 0.8, 0.7, 0.6, 0.5, 0.4],\n",
    "                          'max_depth':[None,2,4,6,8,10,20], \n",
    "                         }, clf=RandomForestClassifier(random_state=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(max_depth=2, max_features=0.5,\n",
    "                       min_impurity_decrease=0.01, min_samples_leaf=10,\n",
    "                       random_state=2) \n",
    "rf_clf.fit(X_train, y_train)  \n",
    "pred5 = rf_clf.predict(X_test) \n",
    "evaluating_model(y_test, pred5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb1 = XGBClassifier(colsample_bytree= 1.0,\n",
    " learning_rate = 0.1,\n",
    " max_depth = 4,\n",
    " n_estimators= 400,\n",
    " subsample= 1.0)  \n",
    "\n",
    "eval_set  = [(X_test, y_test)]\n",
    "\n",
    "xgb1.fit(X_train, y_train,early_stopping_rounds=10, eval_metric=\"logloss\",eval_set=eval_set, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred6 = xgb1.predict(X_test) \n",
    "evaluating_model(y_test, pred6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "# xgb.feature_importances_ \n",
    "plot_importance(xgb1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbdt = GradientBoostingClassifier(n_estimators=200, learning_rate=0.1,max_depth=1,random_state=0) \n",
    "gbdt.fit(X_train, y_train) \n",
    "\n",
    "pred_gdbt = gbdt.predict(X_test) \n",
    "evaluating_model(y_test, pred_gdbt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the best model and saving them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will choose XGboost \n",
    "\n",
    "import joblib \n",
    "joblib.dump(xgb1, 'model.pkl') \n",
    "model = joblib.load('model.pkl' ) \n",
    "model.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
